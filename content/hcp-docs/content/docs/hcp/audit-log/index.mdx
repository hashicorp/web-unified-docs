---
page_title: HCP audit log streaming
description: |-
  This topic provides an overview of the HashiCorp Cloud Platform (HCP) audit log streaming, as well as usage instructions to set up platform and product audit log streaming to Cloudwatch, Datadog, and Splunk Cloud.
---

# HCP audit log streaming

This topic details HashiCorp Cloud Platform’s (HCP) unified audit log streaming capabilities and the process to enable audit log streaming for HCP platform and product events.

## Introduction

Audit logs are a record of system events and corresponding identification data that are typically collected for security compliance measures or to aid in an incident response. In HCP, audit logs capture information about events for the entire [HCP organization](/hcp/docs/hcp/admin/orgs).

HashiCorp Cloud Platform produces two types of audit logs that you can access:

- _Platform audit logs_ track an organization’s interactions with the overall HCP platform, including when users sign-in and create projects.
- _Product audit logs_ track an organization’s interactions with the individual HCP products, such as for HCP Vault Secrets.

You can stream an organization's audit logs to an external security information and event management (SIEM) provider, such as Splunk or AWS Cloudwatch, where you can review them.

You can enable Audit log streaming through:

1. the HCP Portal on an organization's Audit log streaming page
1. HashiCorp’s official HCP Terraform provider, [`hcp_log_streaming_destination`](https://registry.terraform.io/providers/hashicorp/hcp/latest/docs/resources/log_streaming_destination)

Previously, platform audit logs were not directly accessible by HCP users. Product audit logs are available for each HCP product separately.

## Workflow

The overall workflow to enable audit log streaming from HCP to an external security information and event management (SIEM) system consists of the following steps:

1. Prepare destination and retrieve required credentials. This step varies slightly depending on your SIEM system.
1. Configure the audit log streaming destination in [the HCP Portal](https://portal.cloud.hashicorp.com/). You can also use Terraform and the [HashiCorp Cloud Platform (HCP) Provider](https://registry.terraform.io/providers/hashicorp/hcp/latest/docs) to complete this step.
1. Verify the connection. Use the connection test in the HCP UI to generate a log and send it to your SIEM system. Alternatively, take any action in HCP that generates an audit log, such as attempting a login to an HCP Boundary cluster.
1. View the audit log on your external SIEM system to confirm that streaming is properly configured.

## Guidance

The HCP documentation has several resources to help you stream audit logs from HCP.

### Usage documentation

- [Enable audit log streaming to AWS Cloudwatch](/hcp/docs/hcp/audit-log/enable/cloudwatch)
- [Enable audit log streaming to Datadog](/hcp/docs/hcp/audit-log/enable/datadog)
- [Enable audit log streaming to Splunk](/hcp/docs/hcp/audit-log/enable/splunk)

### Reference documentation

- [HCP audit log event and payload reference](/hcp/docs/hcp/audit-log/reference)

## Constraints and limitations

Be aware of the following technical constraints and limitations for HCP audit log streaming:

- You must authenticate to HCP with an organization-level service principal. Authentication with a project-level service principal results in an error.
- When provided with a credential such as a token or API key that is not valid or does not have the correct permissions, HCP does not store logs that it is unable to stream. Logs begin to stream after you apply valid authentication credentials using the HCP UI or the Terraform provider.
- HCP does not process the audit log queue synchronously. It attempts to send logs for 7 days and performs an exponential backoff over that period by increasing the amount of time between attempts.
- Audit log redelivery is also subject to individual SIEM provider constraints. For example, Datadog accepts redelivery requests for logs within 18 hours of their timestamp. For more information about log event requirements, refer to [Custom log forwarding in the Datadog documentation](https://docs.datadoghq.com/logs/log_collection/?tab=host#custom-log-forwarding).