---
page_title: Implement semi-automated deployments
description: Transition from manual to semi-automated deployments using version control, scripting, and immutable infrastructure patterns with Packer and Terraform.
---

# Implement semi-automated deployments

Semi-automated deployments represent the first step in your [process automation journey](/well-architected-framework/define-and-automate-processes), establishing the foundation for advanced automation practices. Once you identify manual deployments in your organization, you can start automating them incrementally. Going from manual to fully automated deployments can be challenging and overwhelming, so organizations commonly migrate to semi-automated deployments before evolving into [fully-automated ones](/well-architected-framework/define-and-automate-processes/process-automation/fully-automated).

This document guides you through implementing version control, scripting deployments, creating immutable infrastructure, and auditing changes - the essential building blocks for automation maturity.

## Why start with semi-automated deployments

Implementing semi-automated deployments first addresses the following operational challenges while building toward full automation:

- **Establish foundation without overwhelming teams:** Jumping directly to fully-automated CI/CD overwhelms teams unfamiliar with automation concepts, infrastructure as code, or version control workflows. Semi-automated deployments let teams learn these fundamentals incrementally, building confidence and skills while delivering immediate value. Teams can adopt version control, then scripting, then immutable infrastructure at a manageable pace.

- **Deliver quick wins that build momentum:** Manual deployments create immediate, visible problems - inconsistent configurations, forgotten steps, deployment failures. Semi-automated practices deliver fast, measurable improvements that demonstrate automation value to stakeholders. Version-controlled scripts eliminate configuration drift, automated image building removes manual errors, and audit logs catch unauthorized changes. These quick wins build organizational support for advancing to full automation.

- **Create reusable assets for future automation:** Scripts, Packer templates, and Terraform configurations you create during semi-automation become the building blocks for CI/CD pipelines. Version-controlled infrastructure code, golden images, and deployment scripts integrate directly into automated workflows. This incremental approach avoids throwaway work - every semi-automated practice you implement moves you closer to full automation.

- **Reduce risk through controlled progression:** Full automation requires significant infrastructure changes - CI/CD platforms, pipeline design, security integration, monitoring setup. Semi-automation lets you validate your automation approach with real workloads before committing to these investments. You discover which processes need automation most, which tools fit your environment, and what challenges emerge, reducing costly mistakes during full automation implementation.

## Use version control systems (VCS)

Version control systems like GitHub or GitLab provide the foundation for automation by tracking every change to your infrastructure code. You should store your automation scripts and IaC in version control systems like GitHub or GitLab. Version control systems increase repeatability and reliability when deploying and configuring infrastructure. Each time you run a script, you should pull it down from your version control system to ensure it is up-to-date since the last time you ran it. You can also version scripts for environments. You can use tags in VCS for each environment, one for development, staging, and production. Storing your Terraform configurations and Packer templates in version control ensures every team member uses the same tested automation scripts, eliminating deployment failures from outdated or modified scripts. 
 
## Use scripts instead of manually running commands

Infrastructure automation through scripting is a fundamental shift that can increase the reliability and security of your infrastructure and application. When you script your infrastructure, you reduce human error, enable scaling, increase consistency, and create a foundation for further automation. 

You can use infrastructure as code tools, such as Terraform and Ansible, to deploy and configure your infrastructure. Infrastructure as code lets you define your infrastructure using code and configuration files instead of manually configuring servers, networks, and other resources. This approach allows you to version, test, and deploy infrastructure changes just like you would with application code, making it easier to maintain consistency, automate deployments, and quickly recover from failures. By treating infrastructure as code, you can ensure your environments are reproducible, scalable, and maintainable. Scripting your deployments with Terraform transforms error-prone manual processes into reliable, repeatable operations that any team member can execute successfully.

## Manage secrets with Vault

Automation scripts require secrets like API keys, database credentials, cloud provider tokens, and encryption keys. Storing these secrets directly in your scripts, environment variables, or configuration files creates security risks and makes credential rotation difficult. When secrets leak into version control or logs, you must rotate all credentials and audit who accessed them.

Vault centralizes secrets management for your automation workflows. Instead of hardcoding credentials in Terraform configurations or Packer templates, your automation tools retrieve secrets from Vault at runtime. When you rotate a database password in Vault, all automation scripts immediately use the updated credential without code changes.

The following example shows how Terraform integrates with Vault to retrieve database credentials:

```hcl
# Configure Vault provider
provider "vault" {
  address = "https://vault.example.com:8200"
}

# Retrieve database credentials from Vault
data "vault_generic_secret" "database" {
  path = "secret/data/production/database"
}

# Use Vault-managed credentials in your infrastructure
resource "aws_db_instance" "app_database" {
  identifier        = "app-db"
  engine            = "postgres"
  instance_class    = "db.t3.micro"
  username          = data.vault_generic_secret.database.data["username"]
  password          = data.vault_generic_secret.database.data["password"]
  allocated_storage = 20
}
```

This Terraform configuration retrieves database credentials from Vault instead of hardcoding them. When you run `terraform apply`, Terraform authenticates to Vault and fetches the current credentials. You rotate passwords in Vault without modifying Terraform code, and Vault's audit logs track which automation scripts access which secrets. This approach eliminates credential sprawl and provides centralized control over secret access.

Packer also integrates with Vault to securely access secrets during image builds. You can retrieve API keys, certificates, and configuration secrets without embedding them in Packer templates or build scripts. 

## Create application images with code

Automated image creation ensures consistent application deployments by packaging dependencies, security patches, and application code into immutable images. You should automate building images that your application runs on. These images contain dependencies, security patches, and the application. Manual configuration can lead to installing incorrect dependencies, missing security patches, and inconsistent application installation and configuration.  

Packer and Dockerfiles create images using IaC and automation, provisioning with scripts and tools like Ansible. Packer is a tool for creating identical machine images for multiple platforms from a single source configuration. Packer is lightweight, runs on every major operating system, and is highly performant, creating machine images for multiple platforms in parallel.

The following example shows a Packer template that creates a golden image with your application pre-installed:

```hcl
# golden-image.pkr.hcl - Creates a golden image with application
source "amazon-ebs" "ubuntu" {
  ami_name      = "golden-web-app-{{timestamp}}"
  instance_type = "t3.micro"
  region        = "us-west-2"
  source_ami_filter {
    filters = {
      name                = "ubuntu/images/*ubuntu-jammy-22.04-amd64-server-*"
      root-device-type    = "ebs"
      virtualization-type = "hvm"
    }
    most_recent = true
    owners      = ["099720109477"]
  }
  ssh_username = "ubuntu"
}

build {
  sources = ["source.amazon-ebs.ubuntu"]

  # Install system dependencies and security patches
  provisioner "shell" {
    inline = [
      "sudo apt-get update",
      "sudo apt-get upgrade -y",
      "sudo apt-get install -y nginx nodejs npm"
    ]
  }

  # Copy application files
  provisioner "file" {
    source      = "dist/"
    destination = "/tmp/app"
  }

  # Configure and install application
  provisioner "shell" {
    inline = [
      "sudo mv /tmp/app /var/www/app",
      "cd /var/www/app && sudo npm install --production",
      "sudo systemctl enable nginx"
    ]
  }
}
```

This Packer template creates a golden image with your web application pre-installed. Running `packer build golden-image.pkr.hcl` produces an AMI (for example, ami-0abc123) that contains Ubuntu with system patches, Nginx, Node.js, and your application files. This immutable image eliminates manual configuration steps and ensures consistent deployments across all environments.

You can use Packer to build a golden image pipeline that creates a reusable base image called a golden image. A golden image is an image on top of which developers can build applications, letting them focus on the application itself instead of system dependencies and patches. A typical golden image includes common system, logging, and monitoring tools, recent security patches, and application dependencies. After Packer builds your application image and outputs an AMI ID, Terraform queries that AMI using a data source and deploys it to your infrastructure. When you update your application, Packer creates a new AMI version. Running `terraform apply` causes Terraform to detect the AMI change, destroy the existing instance, and create a new one running the updated image—implementing immutable infrastructure automatically.  

HCP Packer further simplifies image creation and deployment by providing a central artifact registry for operations and development teams. HCP Packer stores metadata about the artifacts you build, including when you create the artifact, the associated platform, and which Git commit is associated with your build. 

## Create immutable infrastructure

Immutable infrastructure is infrastructure that, once deployed, is never modified, only replaced. For example, in a mutable server, you update the server either by connecting to the server and running commands or by using a script. With immutable infrastructure, you would fully replace the server with a new one. 

Terraform, Packer, and Nomad facilitate immutable infrastructure.

Terraform allows you to define and provision infrastructure environments, such as servers, virtual networks, and IAM roles and policies. When you change your IaC, Terraform updates your infrastructure components by modifying the configuration or destroying and recreating it. To see what changes Terraform will apply, you can run `terraform plan` before running `terraform apply`.

The following example shows how Terraform deploys the Packer-built AMI using a data source to query the most recent image:

```hcl
# main.tf - Deploy instances using the Packer-built AMI
data "aws_ami" "golden_image" {
  most_recent = true
  owners      = ["self"]

  filter {
    name   = "name"
    values = ["golden-web-app-*"]
  }
}

resource "aws_instance" "web_server" {
  ami           = data.aws_ami.golden_image.id
  instance_type = "t3.micro"

  tags = {
    Name        = "web-server"
    Environment = "production"
    Built       = "packer"
  }
}

output "instance_id" {
  value = aws_instance.web_server.id
}
```

This Terraform configuration queries the most recent Packer-built AMI using a data source and deploys it as an EC2 instance. When you update your application, run `packer build` to create a new AMI, then run `terraform apply`. Terraform destroys the old instance and creates a new one with the updated image, implementing immutable infrastructure. This workflow ensures your infrastructure matches your code exactly, eliminating configuration drift.

Packer creates and configures machine images that are ready to run. You can use Terraform to deploy these images and `user_data` or other start-up scripts to ensure services start. With Packer, you can create new images when you need to update your application rather than update the existing infrastructure. 

Nomad orchestrates application workloads across clusters, treating application containers and instances as immutable. When you need to update your application, Nomad replaces running instances with new versions rather than modifying existing ones.

The following example shows a Nomad job that deploys a containerized application using immutable deployment patterns:

```hcl
# web-app.nomad.hcl - Deploy containerized web application
job "web-app" {
  datacenters = ["dc1"]
  type        = "service"

  group "web" {
    count = 3

    task "app" {
      driver = "docker"

      config {
        image = "myregistry/web-app:1.2.0"
        ports = ["http"]
      }

      resources {
        cpu    = 500
        memory = 256
      }

      service {
        name = "web-app"
        port = "http"

        check {
          type     = "http"
          path     = "/health"
          interval = "10s"
          timeout  = "2s"
        }
      }
    }

    network {
      port "http" {
        to = 8080
      }
    }
  }

  update {
    max_parallel = 1
    min_healthy_time = "10s"
    healthy_deadline = "3m"
    auto_revert = true
  }
}
```

This Nomad job deploys three instances of your containerized web application. When you update the container image version and run `nomad job run web-app.nomad.hcl`, Nomad performs a rolling deployment—stopping one old instance, starting a new instance with the updated image, verifying health checks pass, then proceeding to the next instance. The `auto_revert` setting automatically rolls back to the previous version if health checks fail, implementing safe immutable deployments. Nomad integrates with Consul for service discovery and health checking, ensuring only healthy instances receive traffic. 

HashiCorp co-founder and CTO Armon Dadgar explains the differences and trade-offs between mutable and immutable infrastructure. 

<VideoEmbed url="https://www.youtube.com/embed/II4PFe9BbmE"/>

## Enable service discovery with Consul

As you automate deployments, your services need to discover and communicate with each other dynamically. Hardcoding IP addresses or hostnames in configuration files breaks when you deploy new instances, scale services, or replace failed nodes. Manual service registration requires updating configuration files and restarting applications every time infrastructure changes.

Consul provides automatic service discovery and health checking for your automated deployments. When you deploy a new service instance, it registers itself with Consul. Other services query Consul to discover available instances and their health status. When an instance fails health checks, Consul removes it from the service registry, preventing traffic from reaching unhealthy services.

The following example shows how services register with Consul and discover each other:

```hcl
# Service registration configuration
service {
  name = "web-app"
  port = 8080

  check {
    id       = "web-app-health"
    http     = "http://localhost:8080/health"
    interval = "10s"
    timeout  = "1s"
  }
}

service {
  name = "database"
  port = 5432

  check {
    id       = "db-health"
    tcp      = "localhost:5432"
    interval = "10s"
    timeout  = "2s"
  }
}
```

These service definitions register your web application and database with Consul. Consul continuously monitors each service using the defined health checks. Your web application queries Consul's DNS interface or HTTP API to discover healthy database instances: `database.service.consul` resolves to only healthy database servers. When you scale your database or redeploy it, Consul automatically updates service discovery—your application code requires no changes.

Consul integrates with Terraform for infrastructure automation, Nomad for workload orchestration, and Vault for secure service-to-service authentication. This integration enables automated service networking across your entire infrastructure.

## Audit your cloud logs

Cloud audit logs detect manual infrastructure changes that bypass your automation workflows, helping you enforce automation discipline across your team. Once you start automating your infrastructure, you must ensure that infrastructure changes only occur through automation. Tools like AWS CloudTrail, Azure Activity Log, or Google Cloud Audit Logs reveal who makes manual changes through the console, when these changes occur, and what resources change outside of automated processes. These audit logs help you identify team members who need additional training on automation workflows and catch unauthorized infrastructure modifications before they cause production issues.

## Next steps

In this section of Process automation, you learned how to implement semi-automated deployments with version control, scripting, immutable infrastructure, and audit logging. Implement semi-automated deployments is part of the [Define and automate processes pillar](/well-architected-framework/define-and-automate-processes).

Continue your automation journey with the following documents:

- [Define and automate processes](/well-architected-framework/define-and-automate-processes) - Review the complete automation workflow and maturity model
- [Automation maturity model](/well-architected-framework/define-and-automate-processes/process-automation) - Assess your current automation level
- [Implement fully-automated deployments](/well-architected-framework/define-and-automate-processes/process-automation/fully-automated) - Advance to Git-driven CI/CD, self-service infrastructure, and comprehensive monitoring

Related resources for implementing semi-automated practices:

HashiCorp resources:
- Introduction to [Terraform](/terraform/intro), [Packer](/packer/docs/intro), [Vault](/vault/docs/what-is-vault), [Consul](/consul/docs/intro), and [Nomad](/nomad/intro)
- [Build Immutable Infrastructure with Packer in CI/CD](/packer/guides/packer-on-cicd)
- Learn to build a [golden image pipeline with HCP Packer](/packer/tutorials/cloud-production/golden-image-with-hcp-packer)
- [Inject secrets into Terraform using the Vault provider](/terraform/tutorials/secrets/secrets-vault)
- [Store and retrieve dynamic secrets with Vault](/vault/tutorials/get-started-hcp-vault-dedicated/vault-get-started-intro)
- [Register services with Consul](/consul/tutorials/get-started-vms/virtual-machine-gs-service-discovery)
- [Deploy and manage applications with Nomad](/nomad/tutorials/get-started/get-started-install)
- [Identify common metrics](/well-architected-framework/optimize-systems/monitor-system-health/identify-metrics)

External resources:
- [Ansible community documentation](https://docs.ansible.com/?extIdCarryOver=true&sc_cid=701f2000001OH7YAAW)
- Cloud logs documentation for [GCP](https://cloud.google.com/logging/docs/audit), [AWS](https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html), and [Azure](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/data-platform-logs)