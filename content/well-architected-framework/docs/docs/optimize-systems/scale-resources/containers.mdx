---
page_title: Scale container orchestrator and workloads
description: Implement automatic scaling for container orchestrators and workloads to dynamically meet resource requirements and improve application performance.
---

# Scale container orchestrator and workloads

Container orchestrators like Kubernetes and Nomad provide powerful scaling capabilities that allow your applications to automatically adjust to changing demand. Unlike traditional virtual machine scaling, containerized workloads can scale more quickly and offer greater flexibility, enabling you to optimize resource utilization and maintain performance during traffic spikes.

Scaling container workloads effectively requires understanding both horizontal and vertical scaling strategies, implementing proper monitoring and metrics collection, and configuring your orchestrator to make intelligent scaling decisions based on actual application performance.

## Implement horizontal pod autoscaling

Horizontal pod autoscaling allows your container workloads to scale out by adding more instances when demand increases and scale in by removing instances when demand decreases. This approach is ideal for stateless applications that can handle multiple concurrent instances.

Configure your Kubernetes Horizontal Pod Autoscaler (HPA) to monitor CPU and memory utilization, custom metrics, or external metrics from your monitoring system. Set appropriate minimum and maximum replica counts to ensure your application always has sufficient capacity while preventing runaway scaling that could exhaust cluster resources.

For Nomad workloads, use the Nomad Autoscaler to implement similar horizontal scaling capabilities. Configure the autoscaler to monitor resource utilization and scale your job groups based on predefined policies. This enables automatic scaling without manual intervention while maintaining application availability.

## Configure resource limits and requests

Proper resource configuration is essential for effective container scaling. Set resource requests to define the minimum resources your containers need, and set resource limits to prevent containers from consuming excessive resources that could impact other workloads.

Use Terraform to manage your container resource configurations as code. This ensures consistent resource allocation across environments and enables version control for your scaling policies. Define resource requirements in your Terraform configurations rather than hardcoding them in your container manifests.

Monitor actual resource usage patterns to fine-tune your resource requests and limits. Over-provisioning resources wastes capacity, while under-provisioning can lead to performance issues and failed scaling attempts. Use monitoring tools to track CPU, memory, and network utilization to optimize your resource allocation.

## Implement cluster autoscaling

Cluster autoscaling automatically adjusts the size of your container orchestrator cluster based on workload demands. This ensures you have sufficient node capacity to schedule new pods or job allocations while avoiding unnecessary infrastructure costs.

Configure your cloud provider's cluster autoscaler to monitor pending pods or job allocations and automatically provision new nodes when needed. Set appropriate scaling policies that consider factors like node group sizes, scaling delays, and cost optimization strategies.

Use Terraform to manage your cluster autoscaling configurations consistently across environments. Define your node groups, scaling policies, and resource requirements in Terraform to ensure reproducible deployments and easier maintenance.

## Monitor scaling performance

Effective container scaling requires comprehensive monitoring to understand how your applications perform under different load conditions. Implement metrics collection for key performance indicators like response times, throughput, and resource utilization.

Configure alerts for scaling events to ensure you are aware of when and why your applications are scaling. Monitor scaling metrics like scale-up and scale-down frequency, scaling latency, and scaling efficiency to optimize your autoscaling policies.

Use centralized logging to track scaling events and correlate them with application performance. This helps you identify patterns and optimize your scaling configurations for better performance and cost efficiency.

## Next steps

In this section of Scale resources, you learned about implementing automatic scaling for container workloads and orchestrators, including horizontal pod autoscaling, resource configuration, cluster autoscaling, and performance monitoring. Scale container orchestrator and workloads is part of the [Optimize systems](/well-architected-framework/optimize-systems).

Refer to the following documents to learn more about scaling container workloads:

- [Scale servers](/well-architected-framework/optimize-systems/scale-resources/servers) to implement server-level scaling strategies
- [Detect configuration drift](/well-architected-framework/optimize-systems/monitor-system-health/detect-configuration-drift) to maintain consistent configurations across your infrastructure
- [Identify common metrics](/well-architected-framework/optimize-systems/monitor-system-health/identify-metrics) to monitor the right performance indicators

If you are interested in learning more about container orchestration and scaling, you can check out the following resources:

- [Kubernetes provider Horizontal Pod Autoscaler resource](https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs/resources/horizontal_pod_autoscaler) - Terraform documentation for HPA configuration
- [Dynamically Resize with Nomad Autoscaler](/nomad/tutorials/autoscaler) - Guide to implementing autoscaling with Nomad
- [Manage Kubernetes resources via Terraform](/terraform/tutorials/kubernetes/kubernetes-provider) - Tutorial for managing Kubernetes with Terraform
- [Nomad cluster setup with Terraform](/nomad/tutorials/cluster-setup) - Guide to setting up Nomad clusters with Terraform